{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "--- BOLIN PUT YOUR CODE HERE ----\n",
        "<br>\n",
        "--- Data preprocessing and RQ1 ----\n",
        "<br>\n",
        "The following script demonstrates how to automate the download of data using URI from internet to HDFS.(Here the green taxi data of 2019 is used as an example.)"
      ],
      "metadata": {
        "id": "E0fUzbm0UuD5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "I wrote this script to help automate the process of downloading data from the Internet into HDFS\n",
        "and to avoid storing too much data in NFS simutaneously.\n",
        "Execute this program in spark environment, or run it in our own IDE but you should have internet access to the UT spark framework and the path should also be changed accordingly.\n",
        "'''\n",
        "\n",
        "import os\n",
        "import wget\n",
        "import subprocess\n",
        "\n",
        "#Put the urls from which you pull data into NFS system in this list\n",
        "urls = [\"https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2019-01.parquet\",\n",
        "        \"https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2019-02.parquet\",\n",
        "        \"https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2019-03.parquet\",\n",
        "        \"https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2019-04.parquet\",\n",
        "        \"https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2019-05.parquet\",\n",
        "        \"https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2019-06.parquet\",\n",
        "        \"https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2019-07.parquet\",\n",
        "        \"https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2019-08.parquet\",\n",
        "        \"https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2019-09.parquet\",\n",
        "        \"https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2019-10.parquet\",\n",
        "        \"https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2019-11.parquet\",\n",
        "        \"https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2019-12.parquet\"\n",
        "        ]\n",
        "\n",
        "my_nfs_path = \"/home/s3022455\"\n",
        "my_hdfs_path = \"/user/s3022455/taxiData/Green/2019\"\n",
        "\n",
        "for url in urls:\n",
        "    #Use wget to download data into my NFS system. Here I use my own path. If you wanna customizs it you should set it above as your own path.\n",
        "    filename = wget.download(url, out=my_nfs_path)\n",
        "\n",
        "    #Transfer the data from my NFS to my HDFS\n",
        "    subprocess.check_call(['hdfs', 'dfs', '-put', filename, my_hdfs_path])\n",
        "\n",
        "    #Delete the data in NFS\n",
        "    os.remove(filename)"
      ],
      "metadata": {
        "id": "qxu1Y-8Fd0nJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following script returns the first 20 most frequent drop-off locations of green taxi in 2019. For specifically extracting weekday or weekday data, just add a filter method like what the comment shows."
      ],
      "metadata": {
        "id": "qIw9TqHhUbHv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxWFinbKUDXo"
      },
      "outputs": [],
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.sql.session import SparkSession\n",
        "from pyspark.sql.functions import split, col, trim, explode, desc, count\n",
        "from pyspark.sql.types import DoubleType\n",
        "\n",
        "sc = SparkContext(appName = \"Testing\")\n",
        "spark = SparkSession(sc)\n",
        "sc.setLogLevel(\"ERROR\")\n",
        "\n",
        "#df1 = spark.read.json(sc.parallelize([parse_green]))\n",
        "file_path_list = [\"/path/of/my/file1\", \"/path/of/my/file2\", ...\"/path/of/my/fileN\"]\n",
        "\n",
        "#Line 19-28 is to unify \"ehail_fee\" in all 2019 green taxi dataset in DoubleType, or the merge of parquet files will fail.\n",
        "#If all the files have the same data type on each column, you can directly merge it from the code of line 31.\n",
        "dfs = []\n",
        "\n",
        "for file_path in file_path_list:\n",
        "    df = spark.read.parquet(file_path)\n",
        "    df = df.withColumn(\"ehail_fee\", col(\"ehail_fee\").cast(DoubleType()))\n",
        "    dfs.append(df)\n",
        "\n",
        "df1 = dfs[0]\n",
        "for df in dfs[1:]:\n",
        "    df1 = df1.union(df)\n",
        "\n",
        "df1 = spark.read.parquet(*file_path_list, mergeSchema=True)\n",
        "\n",
        "#To tag whether this trip is on weekday or on weekend, very useful I have to say.\n",
        "df2 = df1.withColumn(\"weekend\", ((dayofweek(\"lpep_pickup_datetime\")==1)|(dayofweek(\"lpep_pickup_datetime\")==7)).cast(\"int\"))\n",
        "\n",
        "df3 = df1.drop(\"mta_tax\", \"improvement_surcharge\", \"VendorID\")\n",
        "\n",
        "#Turn the datatype of values in \"dolocationid\" into array so that explode method can be applied\n",
        "df4 = df3.withColumn(\"DOLocationID\", trim(col(\"DOLocationID\"))).withColumn(\"DOLocationID\", split(col(\"DOLocationID\"), \",\"))\n",
        "\n",
        "#df5 = df4.filter(df3.weekend == 1 or 0)\n",
        "\n",
        "df5 = df4.select(explode(\"DOLocationID\").alias(\"DOLocationID\"))\n",
        "\n",
        "df6 = df5.groupBy(\"DOLocationID\").agg(count(col(\"*\")).alias(\"count\"))\n",
        "\n",
        "df7 = df6.orderBy(desc(\"count\")).show(20)\n",
        "\n",
        "df8 = df7.limit(20)\n",
        "\n",
        "df8.write.csv(\"/user/s3022455/Green\")\n",
        "\n",
        "\"\"\"\n",
        "Green taxi 2019 example\n",
        "+------------+------+\n",
        "|DOLocationID| count|\n",
        "+------------+------+\n",
        "|          74|230014|\n",
        "|          42|214976|\n",
        "|          41|190330|\n",
        "|          75|166268|\n",
        "|         129|157254|\n",
        "|           7|155140|\n",
        "|         166|128331|\n",
        "|         181|106210|\n",
        "|          82|104244|\n",
        "|         223| 96710|\n",
        "|          95| 96558|\n",
        "|         236| 96190|\n",
        "|         244| 91202|\n",
        "|         238| 90936|\n",
        "|          61| 90322|\n",
        "|         116| 85612|\n",
        "|          97| 84012|\n",
        "|          49| 72124|\n",
        "|         138| 71428|\n",
        "|         226| 70451|\n",
        "+------------+------+\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "--- HAKAN PUT YOUR CODE HERE ----\n",
        "<br>\n",
        "--- RQ2 ----"
      ],
      "metadata": {
        "id": "xBoCrGVKkq0K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Hakan's code\n",
        "\n",
        "\n",
        "\n",
        "from pyspark.context import SparkContext\n",
        "from pyspark.sql.session import SparkSession\n",
        "from pyspark.sql.functions import col, concat, lit, expr,format_string, hour, asc,input_file_name\n",
        "\n",
        "sc = SparkContext(appName = 'rqProject')\n",
        "sc.setLogLevel(\"ERROR\")\n",
        "spark = SparkSession(sc)\n",
        "\n",
        "#for Green Data: columnsToKeep = ['VendorID','lpep_pickup_datetime','lpep_dropoff_datetime']\n",
        "columnsToKeep = ['hvfhs_license_num','dispatching_base_num', 'request_datetime','pickup_datetime']\n",
        "hdfs_directory_path = '/user/s3022455/'\n",
        "\n",
        "file_paths = [\n",
        "    hdfs_directory_path + 'taxiData/HVFHV/2023/fhvhv_tripdata_2023-01.parquet',\n",
        "    hdfs_directory_path + 'taxiData/HVFHV/2023/fhvhv_tripdata_2023-02.parquet',\n",
        "    hdfs_directory_path + 'taxiData/HVFHV/2023/fhvhv_tripdata_2023-03.parquet',\n",
        "    hdfs_directory_path + 'taxiData/HVFHV/2023/fhvhv_tripdata_2023-04.parquet',\n",
        "    hdfs_directory_path + 'taxiData/HVFHV/2023/fhvhv_tripdata_2023-05.parquet',\n",
        "    hdfs_directory_path + 'taxiData/HVFHV/2023/fhvhv_tripdata_2023-06.parquet',\n",
        "    hdfs_directory_path + 'taxiData/HVFHV/2023/fhvhv_tripdata_2023-07.parquet',\n",
        "    hdfs_directory_path + 'taxiData/HVFHV/2023/fhvhv_tripdata_2023-08.parquet',\n",
        "    hdfs_directory_path + 'taxiData/HVFHV/2023/fhvhv_tripdata_2023-09.parquet',\n",
        "    hdfs_directory_path + 'taxiData/HVFHV/2023/fhvhv_tripdata_2023-10.parquet'\n",
        "]\n",
        "\n",
        "\n",
        "df = spark.read.parquet(*file_paths)\n",
        "df2 = df.select(columnsToKeep)\n",
        "dfWithFileName = df2.withColumn('hvfhv-2019', input_file_name())\n",
        "#for Green Data: df3 = df2.withColumn('hour_range', expr(\"concat(date_format(lpep_pickup_datetime, 'HH:00:00'), ' - ', date_format(lpep_pickup_datetime, 'HH:59:59'))\"))\n",
        "df3 = df2.withColumn('timestamp', expr(\"concat(date_format(request_datetime, 'HH:00:00'), ' - ', date_format(request_datetime, 'HH:59:59'))\"))\n",
        "df4 = df3.groupBy('timestamp').count().sort(col('timestamp'))\n",
        "df4.write.csv('/user/s3105555/project', header=True, mode='overwrite')\n",
        "\n",
        "'''\n",
        "KEEP LINES BELOW, It works for a single file\n",
        "df = spark.read.parquet(/user/s3022455/taxiData/Green/2019/green_tripdata_2019-01.parquet)\n",
        "df2 = df.select(columnsToKeep)\n",
        "df3 = df2.withColumn('hour_range', expr(\"concat(date_format(lpep_pickup_datetime, 'HH:00:00'), ' - ', date_format(lpep_pickup_datetime, 'HH:59:59'))\"))\n",
        "df4 = df3.groupBy('hour_range').count().sort(col('hour_range'))\n",
        "df4.write.csv('/user/s3105555/project', mode = 'overwrite')\n",
        "'''\n",
        "\n",
        "'''\n",
        "2019\n",
        "file_paths = [\n",
        "    hdfs_directory_path + 'taxiData/Green/2019/green_tripdata_2019-01.parquet',\n",
        "    hdfs_directory_path + 'taxiData/Green/2019/green_tripdata_2019-02.parquet',\n",
        "    hdfs_directory_path + 'taxiData/Green/2019/green_tripdata_2019-03.parquet',\n",
        "    hdfs_directory_path + 'taxiData/Green/2019/green_tripdata_2019-04.parquet',\n",
        "    hdfs_directory_path + 'taxiData/Green/2019/green_tripdata_2019-05.parquet',\n",
        "    hdfs_directory_path + 'taxiData/Green/2019/green_tripdata_2019-06.parquet',\n",
        "    hdfs_directory_path + 'taxiData/Green/2019/green_tripdata_2019-07.parquet',\n",
        "    hdfs_directory_path + 'taxiData/Green/2019/green_tripdata_2019-08.parquet',\n",
        "    hdfs_directory_path + 'taxiData/Green/2019/green_tripdata_2019-09.parquet',\n",
        "    hdfs_directory_path + 'taxiData/Green/2019/green_tripdata_2019-10.parquet',\n",
        "    hdfs_directory_path + 'taxiData/Green/2019/green_tripdata_2019-11.parquet',\n",
        "    hdfs_directory_path + 'taxiData/Green/2019/green_tripdata_2019-12.parquet'\n",
        "]\n",
        "\n",
        "2020\n",
        "file_paths = [\n",
        "    hdfs_directory_path + 'taxiData/Green/2020/green_tripdata_2020-01.parquet',\n",
        "    hdfs_directory_path + 'taxiData/Green/2020/green_tripdata_2020-02.parquet',\n",
        "    hdfs_directory_path + 'taxiData/Green/2020/green_tripdata_2020-03.parquet',\n",
        "    hdfs_directory_path + 'taxiData/Green/2020/green_tripdata_2020-04.parquet',\n",
        "    hdfs_directory_path + 'taxiData/Green/2020/green_tripdata_2020-05.parquet',\n",
        "    hdfs_directory_path + 'taxiData/Green/2020/green_tripdata_2020-06.parquet',\n",
        "    hdfs_directory_path + 'taxiData/Green/2020/green_tripdata_2020-07.parquet',\n",
        "    hdfs_directory_path + 'taxiData/Green/2020/green_tripdata_2020-08.parquet',\n",
        "    hdfs_directory_path + 'taxiData/Green/2020/green_tripdata_2020-09.parquet',\n",
        "    hdfs_directory_path + 'taxiData/Green/2020/green_tripdata_2020-10.parquet',\n",
        "    hdfs_directory_path + 'taxiData/Green/2020/green_tripdata_2020-11.parquet',\n",
        "    hdfs_directory_path + 'taxiData/Green/2020/green_tripdata_2020-12.parquet'\n",
        "]\n",
        "\n",
        "2021\n",
        "file_paths = [\n",
        "    hdfs_directory_path + 'taxiData/Green/2021/green_tripdata_2021-01.parquet',\n",
        "    hdfs_directory_path + 'taxiData/Green/2021/green_tripdata_2021-02.parquet',\n",
        "    hdfs_directory_path + 'taxiData/Green/2021/green_tripdata_2021-03.parquet',\n",
        "    hdfs_directory_path + 'taxiData/Green/2021/green_tripdata_2021-04.parquet',\n",
        "    hdfs_directory_path + 'taxiData/Green/2021/green_tripdata_2021-05.parquet',\n",
        "    hdfs_directory_path + 'taxiData/Green/2021/green_tripdata_2021-06.parquet',\n",
        "    hdfs_directory_path + 'taxiData/Green/2021/green_tripdata_2021-07.parquet',\n",
        "    hdfs_directory_path + 'taxiData/Green/2021/green_tripdata_2021-08.parquet',\n",
        "    hdfs_directory_path + 'taxiData/Green/2021/green_tripdata_2021-09.parquet',\n",
        "    hdfs_directory_path + 'taxiData/Green/2021/green_tripdata_2021-10.parquet',\n",
        "    hdfs_directory_path + 'taxiData/Green/2021/green_tripdata_2021-11.parquet',\n",
        "    hdfs_directory_path + 'taxiData/Green/2021/green_tripdata_2021-12.parquet'\n",
        "]\n",
        "\n",
        "2022\n",
        "file_paths = [\n",
        "    hdfs_directory_path + 'taxiData/Green/2022/green_tripdata_2022-01.parquet',\n",
        "    hdfs_directory_path + 'taxiData/Green/2022/green_tripdata_2022-02.parquet',\n",
        "    hdfs_directory_path + 'taxiData/Green/2022/green_tripdata_2022-03.parquet',\n",
        "    hdfs_directory_path + 'taxiData/Green/2022/green_tripdata_2022-04.parquet',\n",
        "    hdfs_directory_path + 'taxiData/Green/2022/green_tripdata_2022-05.parquet',\n",
        "    hdfs_directory_path + 'taxiData/Green/2022/green_tripdata_2022-06.parquet',\n",
        "    hdfs_directory_path + 'taxiData/Green/2022/green_tripdata_2022-07.parquet',\n",
        "    hdfs_directory_path + 'taxiData/Green/2022/green_tripdata_2022-08.parquet',\n",
        "    hdfs_directory_path + 'taxiData/Green/2022/green_tripdata_2022-09.parquet',\n",
        "    hdfs_directory_path + 'taxiData/Green/2022/green_tripdata_2022-10.parquet',\n",
        "    hdfs_directory_path + 'taxiData/Green/2022/green_tripdata_2022-11.parquet',\n",
        "    hdfs_directory_path + 'taxiData/Green/2022/green_tripdata_2022-12.parquet'\n",
        "]\n",
        "\n",
        "2023\n",
        "file_paths = [\n",
        "    hdfs_directory_path + 'taxiData/Green/2023/green_tripdata_2023-01.parquet',\n",
        "    hdfs_directory_path + 'taxiData/Green/2023/green_tripdata_2023-02.parquet',\n",
        "    hdfs_directory_path + 'taxiData/Green/2023/green_tripdata_2023-03.parquet',\n",
        "    hdfs_directory_path + 'taxiData/Green/2023/green_tripdata_2023-04.parquet',\n",
        "    hdfs_directory_path + 'taxiData/Green/2023/green_tripdata_2023-05.parquet',\n",
        "    hdfs_directory_path + 'taxiData/Green/2023/green_tripdata_2023-06.parquet',\n",
        "    hdfs_directory_path + 'taxiData/Green/2023/green_tripdata_2023-07.parquet',\n",
        "    hdfs_directory_path + 'taxiData/Green/2023/green_tripdata_2023-08.parquet',\n",
        "    hdfs_directory_path + 'taxiData/Green/2023/green_tripdata_2023-09.parquet',\n",
        "    hdfs_directory_path + 'taxiData/Green/2023/green_tripdata_2023-10.parquet'\n",
        "]\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "'''\n",
        "hvfhv\n",
        "2019\n",
        "file_paths = [\n",
        "    hdfs_directory_path + 'taxiData/HVFHV/2019/fhvhv_tripdata_2019-02.parquet',\n",
        "    hdfs_directory_path + 'taxiData/HVFHV/2019/fhvhv_tripdata_2019-03.parquet',\n",
        "    hdfs_directory_path + 'taxiData/HVFHV/2019/fhvhv_tripdata_2019-04.parquet',\n",
        "    hdfs_directory_path + 'taxiData/HVFHV/2019/fhvhv_tripdata_2019-05.parquet',\n",
        "    hdfs_directory_path + 'taxiData/HVFHV/2019/fhvhv_tripdata_2019-06.parquet',\n",
        "    hdfs_directory_path + 'taxiData/HVFHV/2019/fhvhv_tripdata_2019-07.parquet',\n",
        "    hdfs_directory_path + 'taxiData/HVFHV/2019/fhvhv_tripdata_2019-08.parquet',\n",
        "    hdfs_directory_path + 'taxiData/HVFHV/2019/fhvhv_tripdata_2019-09.parquet',\n",
        "    hdfs_directory_path + 'taxiData/HVFHV/2019/fhvhv_tripdata_2019-10.parquet',\n",
        "    hdfs_directory_path + 'taxiData/HVFHV/2019/fhvhv_tripdata_2019-11.parquet',\n",
        "    hdfs_directory_path + 'taxiData/HVFHV/2019/fhvhv_tripdata_2019-12.parquet'\n",
        "]\n",
        "\n",
        "2020\n",
        "\n",
        "file_paths = [\n",
        "    hdfs_directory_path + 'taxiData/HVFHV/2020/fhvhv_tripdata_2020-01.parquet',\n",
        "    hdfs_directory_path + 'taxiData/HVFHV/2020/fhvhv_tripdata_2020-02.parquet',\n",
        "    hdfs_directory_path + 'taxiData/HVFHV/2020/fhvhv_tripdata_2020-03.parquet',\n",
        "    hdfs_directory_path + 'taxiData/HVFHV/2020/fhvhv_tripdata_2020-04.parquet',\n",
        "    hdfs_directory_path + 'taxiData/HVFHV/2020/fhvhv_tripdata_2020-05.parquet',\n",
        "    hdfs_directory_path + 'taxiData/HVFHV/2020/fhvhv_tripdata_2020-06.parquet',\n",
        "    hdfs_directory_path + 'taxiData/HVFHV/2020/fhvhv_tripdata_2020-07.parquet',\n",
        "    hdfs_directory_path + 'taxiData/HVFHV/2020/fhvhv_tripdata_2020-08.parquet',\n",
        "    hdfs_directory_path + 'taxiData/HVFHV/2020/fhvhv_tripdata_2020-09.parquet',\n",
        "    hdfs_directory_path + 'taxiData/HVFHV/2020/fhvhv_tripdata_2020-10.parquet',\n",
        "    hdfs_directory_path + 'taxiData/HVFHV/2020/fhvhv_tripdata_2020-11.parquet',\n",
        "    hdfs_directory_path + 'taxiData/HVFHV/2020/fhvhv_tripdata_2020-12.parquet'\n",
        "]\n",
        "\n",
        "2021\n",
        "file_paths = [\n",
        "    hdfs_directory_path + 'taxiData/HVFHV/2021/fhvhv_tripdata_2021-01.parquet',\n",
        "    hdfs_directory_path + 'taxiData/HVFHV/2021/fhvhv_tripdata_2021-02.parquet',\n",
        "    hdfs_directory_path + 'taxiData/HVFHV/2021/fhvhv_tripdata_2021-03.parquet',\n",
        "    hdfs_directory_path + 'taxiData/HVFHV/2021/fhvhv_tripdata_2021-04.parquet',\n",
        "    hdfs_directory_path + 'taxiData/HVFHV/2021/fhvhv_tripdata_2021-05.parquet',\n",
        "    hdfs_directory_path + 'taxiData/HVFHV/2021/fhvhv_tripdata_2021-06.parquet',\n",
        "    hdfs_directory_path + 'taxiData/HVFHV/2021/fhvhv_tripdata_2021-07.parquet',\n",
        "    hdfs_directory_path + 'taxiData/HVFHV/2021/fhvhv_tripdata_2021-08.parquet',\n",
        "    hdfs_directory_path + 'taxiData/HVFHV/2021/fhvhv_tripdata_2021-09.parquet',\n",
        "    hdfs_directory_path + 'taxiData/HVFHV/2021/fhvhv_tripdata_2021-10.parquet',\n",
        "    hdfs_directory_path + 'taxiData/HVFHV/2021/fhvhv_tripdata_2021-11.parquet',\n",
        "    hdfs_directory_path + 'taxiData/HVFHV/2021/fhvhv_tripdata_2021-12.parquet'\n",
        "]\n",
        "\n",
        "2022\n",
        "file_paths = [\n",
        "    hdfs_directory_path + 'taxiData/HVFHV/2022/fhvhv_tripdata_2022-01.parquet',\n",
        "    hdfs_directory_path + 'taxiData/HVFHV/2022/fhvhv_tripdata_2022-02.parquet',\n",
        "    hdfs_directory_path + 'taxiData/HVFHV/2022/fhvhv_tripdata_2022-03.parquet',\n",
        "    hdfs_directory_path + 'taxiData/HVFHV/2022/fhvhv_tripdata_2022-04.parquet',\n",
        "    hdfs_directory_path + 'taxiData/HVFHV/2022/fhvhv_tripdata_2022-05.parquet',\n",
        "    hdfs_directory_path + 'taxiData/HVFHV/2022/fhvhv_tripdata_2022-06.parquet',\n",
        "    hdfs_directory_path + 'taxiData/HVFHV/2022/fhvhv_tripdata_2022-07.parquet',\n",
        "    hdfs_directory_path + 'taxiData/HVFHV/2022/fhvhv_tripdata_2022-08.parquet',\n",
        "    hdfs_directory_path + 'taxiData/HVFHV/2022/fhvhv_tripdata_2022-09.parquet',\n",
        "    hdfs_directory_path + 'taxiData/HVFHV/2022/fhvhv_tripdata_2022-10.parquet',\n",
        "    hdfs_directory_path + 'taxiData/HVFHV/2022/fhvhv_tripdata_2022-11.parquet',\n",
        "    hdfs_directory_path + 'taxiData/HVFHV/2022/fhvhv_tripdata_2022-12.parquet'\n",
        "]\n",
        "'''"
      ],
      "metadata": {
        "id": "-y9tVEp9kyHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "--- YAWEN AND WAN PUT YOUR CODE HERE ----\n",
        "<br>\n",
        "--- RQ3 ----\n",
        "<br>\n",
        "Research Question 3, this code was originally written and run on the cluster using Nano code editor as well as all the libraries mentioned below"
      ],
      "metadata": {
        "id": "sULLSYvyqBK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import col, when, mean, count, min, max\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initialize a Spark session\n",
        "spark = SparkSession.builder.appName(\"GreenTaxiHVFHVAnalysis\").getOrCreate()\n",
        "\n",
        "# Set the log level to ERROR\n",
        "spark.sparkContext.setLogLevel(\"ERROR\")\n",
        "\n",
        "# Define the base paths for the green taxi and HVFHV data\n",
        "base_path_green = \"/user/s3022455/taxiData/Green/\"\n",
        "base_path_hvfhv = \"/user/s3022455/taxiData/HVFHV/\"\n",
        "\n",
        "# Initialize empty DataFrames for the green taxi and HVFHV data by reading the first available dataset\n",
        "green_taxi_df= spark.read.parquet(f\"{base_path_green}2019/*.parquet\")\n",
        "hvfhv_df = spark.read.parquet(f\"{base_path_hvfhv}2019/*.parquet\")\n",
        "\n",
        "# Load data from each subsequent year for both datasets and union with the initial Dataframe\n",
        "for year in range(2019, 2023):  # Up to December 2022\n",
        "    # Read all Parquet files for the current year for green taxi\n",
        "    yearly_data_green = spark.read.parquet(f\"{base_path_green}2019/*.parquet\")\n",
        "    # Read all Parquet files for the current year for HVFHV\n",
        "    yearly_data_hvfhv = spark.read.parquet(f\"{base_path_hvfhv}2019/*.parquet\")\n",
        "\n",
        "    # Union the yearly data with the main DataFrames\n",
        "    green_taxi_df = green_taxi_df.unionByName(yearly_data_green, allowMissingColumns=True)\n",
        "    hvfhv_df = hvfhv_df.unionByName(yearly_data_hvfhv, allowMissingColumns=True)\n",
        "\n",
        "# Show the first few rows of the green_taxi_df DataFrame\n",
        "print(\"Green Taxi DataFrame:\")\n",
        "green_taxi_df.show()\n",
        "\n",
        "# Show the first few rows of the hvfhv_df DataFrame\n",
        "print(\"HVFHV DataFrame:\")\n",
        "hvfhv_df.show()\n",
        "\n",
        "# Select only the 'trip_distance' and 'trip_miles'  column and convert miles to kilometers if necessary\n",
        "green_taxi_df = green_taxi_df.select(\n",
        "    (col(\"trip_distance\") * 1.60934).alias(\"trip_distance_km\")\n",
        ")\n",
        "\n",
        "hvfhv_df = hvfhv_df.select(\n",
        "    (col(\"trip_miles\") * 1.60934).alias(\"trip_distance_km\")\n",
        ")\n",
        "\n",
        "# REMOVING OUTLIERS\n",
        "\n",
        "# Define reasonable limits for trip distances\n",
        "MIN_DISTANCE = 0  # minimum distance in km\n",
        "MAX_DISTANCE = 500  # maximum distance in km\n",
        "\n",
        "# Clean and filter Green Taxi DataFrame\n",
        "green_taxi_df_clean = green_taxi_df.filter(\n",
        "    (col(\"trip_distance_km\") > MIN_DISTANCE) &\n",
        "    (col(\"trip_distance_km\") <= MAX_DISTANCE)\n",
        ")\n",
        "\n",
        "# Clean and filter HVFHV DataFrame\n",
        "hvfhv_df_clean = hvfhv_df.filter(\n",
        "    (col(\"trip_miles\") > MIN_DISTANCE) &\n",
        "    (col(\"trip_miles\") <= MAX_DISTANCE)\n",
        ")\n",
        "\n",
        "# Define the distance thresholds for categorizing trips\n",
        "short_distance_threshold = 5  # km\n",
        "long_distance_threshold = 15  # km\n",
        "\n",
        "# Create a new categorical column 'trip_type_category' in both DataFrames\n",
        "green_taxi_df = green_taxi_df_clean.withColumn(\n",
        "    'trip_type_category',\n",
        "    when(col('trip_distance_km') < short_distance_threshold, 'short')\n",
        "    .when(col('trip_distance_km') <= long_distance_threshold, 'medium')\n",
        "    .otherwise('long')\n",
        ")\n",
        "\n",
        "hvfhv_df = hvfhv_df_clean.withColumn(\n",
        "    'trip_type_category',\n",
        "    when(col('trip_distance_km') < short_distance_threshold, 'short')\n",
        "    .when(col('trip_distance_km') <= long_distance_threshold, 'medium')\n",
        "    .otherwise('long')\n",
        ")\n",
        "\n",
        "# Check for null values in trip_distance for green taxis\n",
        "green_taxi_nulls = green_taxi_df.filter(col(\"trip_distance\").isNull()).count()\n",
        "print(f\"Number of null values in trip_distance for green taxis: {green_taxi_nulls}\")\n",
        "\n",
        "# Remove rows with null values in trip_distance for green taxis\n",
        "green_taxi_df = green_taxi_df.filter(col(\"trip_distance\").isNotNull())\n",
        "\n",
        "# Check for null values in trip_distance for HVFHV\n",
        "hvfhv_nulls = hvfhv_df.filter(col(\"trip_miles\").isNull()).count()\n",
        "print(f\"Number of null values in trip_distance for HVFHV: {hvfhv_nulls}\")\n",
        "\n",
        "# Remove rows with null values in trip_distance for HVFHV\n",
        "hvfhv_df = hvfhv_df.filter(col(\"trip_miles\").isNotNull())\n",
        "\n",
        "# DATA SEGMENTATION\n",
        "\n",
        "# Segment the data based on the trip type categories\n",
        "green_taxi_short_trips = green_taxi_df.filter(col('trip_type_category') == 'short')\n",
        "green_taxi_medium_trips = green_taxi_df.filter(col('trip_type_category') == 'medium')\n",
        "green_taxi_long_trips = green_taxi_df.filter(col('trip_type_category') == 'long')\n",
        "\n",
        "hvfhv_short_trips = hvfhv_df.filter(col('trip_type_category') == 'short')\n",
        "hvfhv_medium_trips = hvfhv_df.filter(col('trip_type_category') == 'medium')\n",
        "hvfhv_long_trips = hvfhv_df.filter(col('trip_type_category') == 'long')\n",
        "\n",
        "# Show the count for each segment\n",
        "print(\"Green Taxi Short Trips Count:\", green_taxi_short_trips.count())\n",
        "print(\"Green Taxi Medium Trips Count:\", green_taxi_medium_trips.count())\n",
        "print(\"Green Taxi Long Trips Count:\", green_taxi_long_trips.count())\n",
        "print(\"HVFHV Short Trips Count:\", hvfhv_short_trips.count())\n",
        "print(\"HVFHV Medium Trips Count:\", hvfhv_medium_trips.count())\n",
        "print(\"HVFHV Long Trips Count:\", hvfhv_long_trips.count())\n",
        "\n",
        "# DESCRIPTIVE ANALYSIS\n",
        "\n",
        "green_taxi_df.show()\n",
        "\n",
        "# Calculate overall mean and median trip distance for Green Taxis\n",
        "green_taxi_overall_stats = green_taxi_df.agg(\n",
        "    F.count(\"trip_distance_km\").alias(\"total_num_trips\"),\n",
        "    F.mean(\"trip_distance_km\").alias(\"overall_mean_distance\"),\n",
        "    F.percentile_approx(\"trip_distance_km\", 0.5).alias(\"overall_median_distance\")\n",
        ")\n",
        "\n",
        "# Calculate overall mean and median trip distance for HVFHV\n",
        "hvfhv_overall_stats = hvfhv_df.agg(\n",
        "    F.count(\"trip_distance_km\").alias(\"total_num_trips\"),\n",
        "    F.mean(\"trip_distance_km\").alias(\"overall_mean_distance\"),\n",
        "    F.percentile_approx(\"trip_distance_km\", 0.5).alias(\"overall_median_distance\")\n",
        ")\n",
        "\n",
        "# Find the min and max values for the Green Taxi DataFrame\n",
        "green_min_max = green_taxi_df.agg(\n",
        "    min(\"trip_distance_km\").alias(\"min_distance\"),\n",
        "    max(\"trip_distance_km\").alias(\"max_distance\")\n",
        ").collect()\n",
        "\n",
        "green_min_distance = green_min_max[0]['min_distance']\n",
        "green_max_distance = green_min_max[0]['max_distance']\n",
        "\n",
        "print(f\"Green Taxi - Min Distance: {green_min_distance}, Max Distance: {green_max_distance}\")\n",
        "\n",
        "# Find the min and max values for the HVFHV DataFrame\n",
        "hvfhv_min_max = hvfhv_df.agg(\n",
        "    min(\"trip_distance_km\").alias(\"min_distance\"),\n",
        "    max(\"trip_distance_km\").alias(\"max_distance\")\n",
        ").collect()\n",
        "\n",
        "hvfhv_min_distance = hvfhv_min_max[0]['min_distance']\n",
        "hvfhv_max_distance = hvfhv_min_max[0]['max_distance']\n",
        "\n",
        "print(f\"HVFHV - Min distance: {hvfhv_min_distance}, Max distance: {hvfhv_max_distance}\")\n",
        "\n",
        "# Show the overall stats for Green Taxis\n",
        "print(\"Overall Green Taxi Stats:\")\n",
        "green_taxi_overall_stats.show()\n",
        "\n",
        "# Show the overall stats for HVFHV\n",
        "print(\"Overall HVFHV Stats:\")\n",
        "hvfhv_overall_stats.show()\n",
        "\n",
        "# PLOTTING\n",
        "\n",
        "# get the counts for each category\n",
        "green_short_count = green_taxi_short_trips.count()\n",
        "green_medium_count = green_taxi_medium_trips.count()\n",
        "green_long_count = green_taxi_long_trips.count()\n",
        "\n",
        "hvfhv_short_count = hvfhv_short_trips.count()\n",
        "hvfhv_medium_count = hvfhv_medium_trips.count()\n",
        "hvfhv_long_count = hvfhv_long_trips.count()\n",
        "\n",
        "# create lists of counts and labels for the pie charts\n",
        "green_counts = [green_short_count, green_medium_count, green_long_count]\n",
        "hvfhv_counts = [hvfhv_short_count, hvfhv_medium_count, hvfhv_long_count]\n",
        "labels = ['Short', 'Medium', 'Long']\n",
        "colors = ['#ff9999', '#66b3ff', '#99ff99']\n",
        "\n",
        "# Green Taxi Pie Chart\n",
        "fig1, ax1 = plt.subplots()\n",
        "ax1.pie(green_counts, colors=colors, labels=labels, autopct='%.2f%%', startangle=90)\n",
        "centre_circle = plt.Circle((0, 0), 0.70, fc='white')\n",
        "fig = plt.gcf()\n",
        "fig.gca().add_artist(centre_circle)\n",
        "ax1.axis('equal')\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(top=0.85)\n",
        "plt.title('Green Taxi - Segmentation of Trips', pad=20)\n",
        "plt.savefig('/home/s2819201/green_taxi_trip_segmentation.png', bbox_inches='tight', dpi=300)\n",
        "plt.close(fig)  # Close the figure to free up memory\n",
        "\n",
        "# HVFHV Pie Chart\n",
        "fig1, ax1 = plt.subplots()\n",
        "ax1.pie(hvfhv_counts, colors=colors, labels=labels, autopct='%.2f%%', startangle=90)\n",
        "centre_circle = plt.Circle((0, 0), 0.70, fc='white')\n",
        "fig = plt.gcf()\n",
        "fig.gca().add_artist(centre_circle)\n",
        "ax1.axis('equal')\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(top=0.85)\n",
        "plt.title('HVFHV - Segmentation of Trips', pad=20)\n",
        "plt.savefig('/home/s2819201/hvfhv_trip_segmentation.png', bbox_inches='tight', dpi=300)\n",
        "plt.close(fig)  # Close the figure to free up memory\n",
        "\n",
        "# Define a range for sampling\n",
        "green_min_sample = 0\n",
        "green_max_sample = 50\n",
        "HVFHV_min_sample = 0\n",
        "HVFHV_max_sample = 50\n",
        "\n",
        "# Sampling 8% of the data for Green Taxi\n",
        "green_sample = green_taxi_df.filter(\n",
        "    (col(\"trip_distance_km\") >= green_min_sample) & (col(\"trip_distance_km\") <= green_max_sample)\n",
        ").sample(False, 0.08).select('trip_distance_km').rdd.flatMap(lambda x: x).collect()\n",
        "\n",
        "# Plot the histogram for Green Taxi\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.hist(green_sample, bins=50, color='green', alpha=0.7)\n",
        "\n",
        "# Lines for threshold, mean, and median\n",
        "threshold_short = 5  # km for short trips\n",
        "threshold_long = 15  # km for long trips\n",
        "green_mean_distance = green_taxi_df.select(mean('trip_distance_km')).collect()[0][0]\n",
        "green_median_distance = green_taxi_df.approxQuantile('trip_distance_km', [0.5], 0.05)[0]\n",
        "\n",
        "plt.axvline(threshold_short, color='yellow', linestyle='dashed', linewidth=1, label=f'Short Threshold: {threshold_short} km')\n",
        "plt.axvline(threshold_long, color='red', linestyle='dashed', linewidth=1, label=f'Long Threshold: {threshold_long} km')\n",
        "plt.axvline(green_mean_distance, color='blue', linestyle='dashed', linewidth=1, label=f'Mean: {green_mean_distance:.2f} km')\n",
        "plt.axvline(green_median_distance, color='orange', linestyle='dashed', linewidth=1, label=f'Median: {green_median_distance:.2f} >\n",
        "plt.title('Green Taxi Trip Distance Distribution')\n",
        "plt.xlabel('Distance (km)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()\n",
        "\n",
        "#Save the figure for Green Taxi\n",
        "plt.savefig('/home/s2819201/green_taxi_trip_distance_distribution.png')\n",
        "plt.close()\n",
        "\n",
        "# Sampling 8% of the data for HVFHV\n",
        "hvfhv_sample = hvfhv_df.filter(\n",
        "    (col(\"trip_distance_km\") >= HVFHV_min_sample) & (col(\"trip_distance_km\") <= HVFHV_max_sample)\n",
        ").sample(False, 0.08).select('trip_distance_km').rdd.flatMap(lambda x: x).collect()\n",
        "\n",
        "# Plot the histogram for HVFHV\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.hist(hvfhv_sample, bins=50, color='blue', alpha=0.7)\n",
        "\n",
        "# Lines for threshold, mean, and median\n",
        "hvfhv_mean_distance = hvfhv_df.select(mean('trip_distance_km')).collect()[0][0]\n",
        "hvfhv_median_distance = hvfhv_df.approxQuantile('trip_distance_km', [0.5], 0.05)[0]\n",
        "\n",
        "plt.axvline(threshold_short, color='yellow', linestyle='dashed', linewidth=1, label=f'Short Threshold: {threshold_short} km')\n",
        "plt.axvline(threshold_long, color='red', linestyle='dashed', linewidth=1, label=f'Long Threshold: {threshold_long} km')\n",
        "plt.axvline(hvfhv_mean_distance, color='green', linestyle='dashed', linewidth=1, label=f'Mean: {hvfhv_mean_distance:.2f} km')\n",
        "plt.axvline(hvfhv_median_distance, color='orange', linestyle='dashed', linewidth=1, label=f'Median: {hvfhv_median_distance:.2f} >\n",
        "\n",
        "# Add title and labels\n",
        "plt.title('HVFHV Trip Distance Distribution')\n",
        "plt.xlabel('Distance (km)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()\n",
        "\n",
        "# Save the figure for HVFHV\n",
        "plt.savefig('/home/s2819201/hvfhv_trip_distance_distribution.png')\n",
        "plt.close()\n",
        "\n",
        "# Stop the Spark Session\n",
        "spark.stop()\n",
        "\n",
        "\"\"\"\n",
        "Number of null values in trip_distance for green taxis: 0\n",
        "Number of null values in trip_distance for HVFHV: 0\n",
        "Green Taxi Short Trips Count: 19696055\n",
        "Green Taxi Medium Trips Count: 8376030\n",
        "Green Taxi Long Trips Count: 2646040\n",
        "HVFHV Short Trips Count: 619494890\n",
        "HVFHV Medium Trips Count: 402896060\n",
        "HVFHV Long Trips Count: 148189580\n",
        "+------------------+------------------+\n",
        "|  trip_distance_km|trip_type_category|\n",
        "+------------------+------------------+\n",
        "|         1.3840324|             short|\n",
        "|1.0621644000000001|             short|\n",
        "|         4.3130312|             short|\n",
        "|         7.2903102|            medium|\n",
        "|          1.689807|             short|\n",
        "|         6.0672118|            medium|\n",
        "| 6.598293999999999|            medium|\n",
        "|         12.472385|            medium|\n",
        "| 5.922371200000001|            medium|\n",
        "|        11.0078856|            medium|\n",
        "|1.8507409999999997|             short|\n",
        "|         0.7885766|             short|\n",
        "| 5.809717399999999|            medium|\n",
        "|1.9312079999999998|             short|\n",
        "|           0.80467|             short|\n",
        "|           8.85137|            medium|\n",
        "|         8.0627934|            medium|\n",
        "|         0.6920162|             short|\n",
        "|         4.3774048|             short|\n",
        "|3.4600809999999997|             short|\n",
        "+------------------+------------------+\n",
        "only showing top 20 rows\n",
        "\n",
        "Green Taxi - Min Distance: 0.0160934, Max Distance: 325.247614\n",
        "HVFHV - Min distance: 0.0016093400000000001, Max distance: 794.0322626\n",
        "Overall Green Taxi Stats:\n",
        "+---------------+---------------------+-----------------------+\n",
        "|total_num_trips|overall_mean_distance|overall_median_distance|\n",
        "+---------------+---------------------+-----------------------+\n",
        "|       30718125|    5.858048259059208|              3.4118008|\n",
        "+---------------+---------------------+-----------------------+\n",
        "\n",
        "Overall HVFHV Stats:\n",
        "+---------------+---------------------+-----------------------+\n",
        "|total_num_trips|overall_mean_distance|overall_median_distance|\n",
        "+---------------+---------------------+-----------------------+\n",
        "|     1170580530|    7.633885356374852|              4.6509926|\n",
        "+---------------+---------------------+-----------------------+\n",
        "\n",
        "generated plotting files:\n",
        "green_taxi_trip_distance_distribution.png\n",
        "green_taxi_trip_segmentation.png\n",
        "hvfhv_trip_distance_distribution.png\n",
        "hvfhv_trip_segmentation.png\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "zSitnO5Vk31m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "s2rkNwhmn4Hx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}